{% set name = "datashader" %}
{% set version = "0.6.6" %}
{% set sha256 = "583fa37fec94de59a8910097430fb0157927020448910ea78be4bc77899471e9" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.zip
  sha256: {{ sha256 }}

build:
  noarch: python
  number: 0
  script: python -m pip install --no-deps --ignore-installed .
  entry_points:
    - datashader = datashader.__main__:main

requirements:
  build:
    - python
    - pip
    - param >=1.6.1
  run:
    - python >=2.7
    - bokeh
    - colorcet >=0.9.0
    - dask >=0.15.4
    - datashape >=0.5.1
    - numba >=0.37.0
    - numpy >=1.7
    - pandas >=0.20.3
    - param >=1.6.1
    - pillow >=3.1.1
    - pyct
    - scikit-image
    - scipy
    - toolz >=0.7.4
    - xarray >=0.9.6

test:
  requires:
    - nbsmoke >=0.2.6
    - pytest >=2.8.5
    - pytest-benchmark >=3.0.0
  imports:
    - datashader
  commands:
    - pytest --pyargs --doctest-modules --doctest-ignore-import-errors datashader
    - datashader copy-examples --path=. --force
    - datashader fetch-data --path=. --datasets small.yml
    # just run one notebook for now; increase in the future if notebooks can be run quickly with test/tiny data
    - pytest --nbsmoke-run -k ".ipynb" getting_started/2_Pipeline.ipynb

about:
  home: https://github.com/bokeh/datashader
  license: New BSD
  license_file: LICENSE.txt
  summary: 'Data visualization toolchain based on aggregating into a grid'

  description: |
    Datashader is a data rasterization pipeline for automating the process of
    creating meaningful representations of large amounts of data.
  doc_url: http://datashader.org
  dev_url: https://github.com/bokeh/datashader

extra:
  recipe-maintainers:
    - jbednar
    - jbcrail
